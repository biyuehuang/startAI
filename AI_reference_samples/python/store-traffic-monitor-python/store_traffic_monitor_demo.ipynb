{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Traffic Monitor\n",
    "\n",
    "Store traffic monitor is an advanced use-case example, which builds on the two previous examples (<a href=\"../flaw-detector-python/flawdetector-demo.ipynb\">flaw detector</a> and <a href=\"../object-detection-python/object_detection_demo.ipynb\">object detector</a>) and shows how a user can perform object detection and inference on multiple videos simultaneously. In this use case, the application monitors the activity of people inside and outside an imaginary store as well as keeps track of product inventory using a pre-trained neural network for detection. This application is capable of detecting objects on any number of screens as long as sufficient compute power is available.\n",
    "\n",
    "This demo assumes you have already tried the <a href=\"../object-detection-python/object_detection_demo.ipynb\">object detector</a> sample. If you have not done so already, we recommend going through that sample first to understand the workflow for developing a deep learning application using OpenVINO. \n",
    "\n",
    "More information on using the store traffic monitor, as well as the original source code, can be found on GitHub:\n",
    "\n",
    "https://github.com/intel-iot-devkit/store-traffic-monitor-python\n",
    "\n",
    "Note that the code in this notebook has been slightly modified from the version on GitHub in order to work in the IoT DevCloud environment.\n",
    "\n",
    "## Overview of How it works?\n",
    "\n",
    "The counter uses the inference engine included in the OpenVINO™ toolkit. A trained neural network detects objects within a designated area and displays a green bounding box over them. This reference implementation identifies multiple intruding objects entering the frame and identifies their class, count, and time entered. \n",
    "\n",
    "At start-up, the sample application reads a configuration file and loads a network and images from the video input to the Inference Engine (IE) plugin. At runtime, computations are offloaded to a the hardware accelerator (Intel® Core CPU, Intel® HD Graphics GPU, Intel® Xeon CPU, Intel® Movidius™ and/or Neural Compute Stick). Once the inference is completed, the output videos are appropriately rendered and stored in the /results directory, which can then be viewed within the Jupyter Notebook instance\n",
    "\n",
    "## Demonstration objectives\n",
    "* Video as input is supported using **OpenCV**\n",
    "* Inference performed on edge compute nodes with various target compute devices\n",
    "* **OpenCV** provides the bounding boxes, labels and other information\n",
    "* Visualization of the resulting bounding boxes\n",
    "* Demonstrate the Async API in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup\n",
    "\n",
    "Import dependencies for the notebook by running the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path().resolve().parent.parent))\n",
    "from demoTools.demoutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Advanced OpenVINO\n",
    "\n",
    "The complete listing of source code for this example is in <a href=\"store_traffic_monitor.py\">store_traffic_monitor.py</a>.\n",
    "This application takes multiple video streams as inputs. The application is designed to work in both sync and async mode. In sync mode, the processing is done by the compute device by taking one frame at a time, whereas in async mode, the device takes multiple frames and processes them in parallel.\n",
    "\n",
    "\n",
    "**Configure the Application**\n",
    "\n",
    "All the configurations are written to store-traffic-monitor-python/conf_fp*.txt. We have two configuration files: conf_fp16.txt and conf_fp32.txt  \n",
    "\n",
    "- **conf_fp16.txt**: use if you want the inference to be run on Intel® Movidius™ Neural Compute Stick as it offers native FP16 precision.\n",
    "- **conf_fp32.txt**: use if you want the inference to be run on Intel® Core CPU, Intel® HD Graphics GPU and Intel® Xeon CPU.\n",
    "\n",
    "1st line: path/to/model.xml -> \n",
    "This is the path to the model topology in the local file system.\n",
    "The model topology file is the .xml file that the model optimizer produces, containing the IR of the model's topology.\n",
    "\n",
    "2nd line: path/to/model.bin ->\n",
    "This is the path to the model weights in the local file system.\n",
    "The model weights file is the .bin file that the model optimizer produces, containing the IR of the model's weights.\n",
    "\n",
    "3rd line: path/to/labels ->\n",
    "This is a path to the labels file in the local file system.\n",
    "The labels file is a text file containing all the classes/labels that the model can recognize, in the order that it was trained to recognize them (one class per line). All detection models work with integer labels and not string labels (e.g. for the mobilenet-ssd model, the **number 15 represents the class \"person\"**).\n",
    "For the mobilenet-ssd model, we provide the class file labels.txt in the **\"/data/reference-sample-data/store-traffic-monitor-python\"** folder.\n",
    "\n",
    "**Note** : store_traffic_monitor application is used to detect a person and a bottle in a frame. For the mobilenet-ssd model used in this example, the line number 5 and  line number 15 in the labels.txt file correspond to bottle and person, respectively. If you are using a different model, then the items in labels.txt file would need to be adjusted appropriately.\n",
    "\n",
    "Each of the following lines contain the path/to/video followed by the label to be detected on that video, e.g.:\n",
    "\n",
    "**/data/reference-sample-data/store-traffic-monitor-python/people-detection.mp4** person\n",
    "\n",
    "The path/to/video is the path, in the local file system, to a video that is used as input. The labels used must coincide with the labels from the labels file.\n",
    "\n",
    "**Models to Use ?**\n",
    "\n",
    "The application works with any object-detection model, provided it has the same input and output format of the mobilenet-ssd model.\n",
    "The model can be any object detection model:\n",
    "\n",
    "* that is provided by OpenVINO™ toolkit.\n",
    "These can be found in the openvino/deployment_tools/intel_models folder.\n",
    "* downloaded using the model downloader.\n",
    "These can be found in the openvino/deployment_tools/model_downloader/object_detection folder.\n",
    "* created by the user\n",
    "\n",
    "For first-time use, we recommend using the mobilenet-ssd model provided as a part of this application. The models can be found under the folder \"/data/reference-sample-data/models/mobilenet-ssd\"\n",
    "\n",
    "**Videos to Use ?**\n",
    "\n",
    "The application works with any input video. Sample videos for object detection are provided here.\n",
    "\n",
    "For first-use, we recommend using the people-detection, one-by-one-person-detection, bottle-detection videos.\n",
    "E.g.:\n",
    "\n",
    "/data/reference-sample-data/store-traffic-monitor-python/people-detection.mp4 person<br>\n",
    "/data/reference-sample-data/store-traffic-monitor-python/one-by-one-person-detection.mp4 person<br>\n",
    "/data/reference-sample-data/store-traffic-monitor-python/bottle-detection.mp4 bottle\n",
    "\n",
    "For the sake of simplicity, the videos are copied into /data/reference-sample-data/store-traffic-monitor-python/ folder\n",
    "\n",
    "**Command line arguments options and how they are interpreted in the application source code**\n",
    "\n",
    "```python\n",
    "** python3 store_traffic_monitor.py -d CPU -c conf_fp32.txt -o results **\n",
    "```\n",
    "\n",
    "#### The description of the arguments used in the argument parser is the command line executable equivalent.\n",
    "\n",
    " *  -o  location where the output file with inference results needs to be stored. (results/core or results/xeon or results/gpu or results/myriad)\n",
    "\n",
    " *  -d Type of Hardware Acceleration (CPU or GPU or MYRIAD)\n",
    " *  -c configuration file to use (\"conf_fp32.txt\" for Intel® Core CPU, Intel® HD Graphics GPU and Intel® Xeon CPU. \"conf_fp16.txt\" for Intel® Movidius™ Neural Compute Stick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Choosing Device\n",
    "First, we must select the device used for inferencing. This is done by loading the appropriate plugin to initialize the specified device (CPU, GPU, MYRIAD etc) and load the extensions library (if specified) provided in the extension/ folder for the device.\n",
    "\n",
    "The plugin class is IEPlugin and can be constructed as follows:\n",
    "```python\n",
    "    # Parses the configuration file \"conf.txt\" and reads model_xml, model_bin, labels_file, videoCaps\n",
    "    parse_conf_file()\n",
    "    # Plugin initialization for specified device and load extensions library\n",
    "    print(\"Initializing plugin for {} device...\".format(TARGET_DEVICE))\n",
    "    plugin = IEPlugin(device=TARGET_DEVICE)\n",
    "    if 'CPU' in TARGET_DEVICE:\n",
    "        plugin.add_cpu_extension(CPU_EXTENSION)\n",
    "```\n",
    "\n",
    "**Note**: Currently, three types of plugins are supported: CPU, GPU, and MYRIAD. The CPU plugin may require additional extensions to improve performance. Use add_cpu_extension function to load these additional extensions.\n",
    "\n",
    "### 1.2 Read the IR (Intermediate Representation) model\n",
    "\n",
    "Intel Model Optimizer creates Intermediate Representation (IR) models that are optimized for different Intel hardware.\n",
    "We can import these optimized models (weights) into our neural network using **`IENetwork`**. \n",
    "```python\n",
    "    net = IENetwork.from_ir(model=model_xml, weights=model_bin)\n",
    "    assert len(net.inputs.keys()) == 1, \"Sample supports only single input topologies\"\n",
    "    assert len(net.outputs) == 1, \"Sample supports only single output topologies\"\n",
    "    input_blob = next(iter(net.inputs))\n",
    "    out_blob = next(iter(net.outputs))\n",
    "```\n",
    "\n",
    "### 1.3 Load the network into the plugin\n",
    "\n",
    "Once we have the plugin and the network, we can load the IR into the plugin using **`plugin.load`**.\n",
    "\n",
    "```python\n",
    "    # Loading IR model to the plugin.\n",
    "    exec_net = plugin.load(network=net, num_requests=2)\n",
    "    # Read and pre-process input image\n",
    "    n, c, h, w = net.inputs[input_blob]\n",
    "    del net\n",
    "\n",
    "```\n",
    "\n",
    "### 1.4 Start video capture using OpenCV \n",
    "\n",
    "Now we are ready to capture the frames from the video sample using **OpenCV VideoCapture** API.\n",
    "Upon getting the frame we are ready to perform inference.\n",
    "\n",
    "```python\n",
    "cap = cv2.VideoCapture(input_stream)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.5 Prepare the Model\n",
    "\n",
    "Run the cells below to download the mobilenet-ssd model and convert it using the Intel Model Optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --print_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User might have problem to download models from google\n",
    "#!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name mobilenet-ssd -o raw_models\n",
    "\n",
    "#Models are downloaded to /raw_models folder already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model raw_models/object_detection/common/mobilenet-ssd/caffe/mobilenet-ssd.caffemodel \\\n",
    "--data_type FP32 \\\n",
    "-o models/mobilenet-ssd/FP32 \\\n",
    "--scale 256 \\\n",
    "--mean_values [127,127,127] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model raw_models/object_detection/common/mobilenet-ssd/caffe/mobilenet-ssd.caffemodel \\\n",
    "--data_type FP16 \\\n",
    "-o models/mobilenet-ssd/FP16 \\\n",
    "--scale 256 \\\n",
    "--mean_values [127,127,127] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Running on the IoT DevCloud\n",
    "\n",
    "### 2.1 Creating job file\n",
    "\n",
    "Like in the object detection example, we will perform inference on multiple edge compute devices by submitting jobs into the IoT DevCloud queue. To do this, first, we need to create a job file.\n",
    "The job file is a Bash script, and will be executed on the edge compute node.\n",
    "For this example, we have written the job file for you in the notebook.\n",
    "Run the following cell to write this in to the file \"store_traffic_manager_job.sh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  -n : number of videos to process\n",
    "\n",
    "!python3 store_traffic_monitor.py  -d CPU \\\n",
    "                                    -m models/mobilenet-ssd/FP32/mobilenet-ssd.xml \\\n",
    "                                    -l labels.txt \\\n",
    "                               #     -e /opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_avx2.so \\\n",
    "                                    -lp false \\\n",
    "                                    -o results\\\n",
    "                                    -c conf_fp32.txt \\\n",
    "                                    -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wait!**\n",
    "\n",
    "Before moving to step 3, make sure that all the monitor_*  jobs submitted to the queue are completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Results\n",
    "\n",
    "Once the jobs are complete, the queue software writes the stdout and stderr streams into files with names of the form\n",
    "\n",
    "`monitor_{type}.o{JobID}`\n",
    "\n",
    "`monitor_{type}.e{JobID}`\n",
    "\n",
    "(here, the prefix `monitor_{type}` is based on our `-N` argument of `qsub`)\n",
    "\n",
    "However, for our store monitor example, rather than studying stdout/stderr, you will likely be more interested in viewing the main output in the mp4 videos that are stored in the `results/` directory.\n",
    "We wrote a short utility script that will display these videos in the notebook. Run the cells below to view the video.\n",
    "See `demoutils.py` if interested in the script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the following four cells to see the output  from out jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('Intel® Core CPU',\n",
    "          ['results/inference_output_Video_0.mp4',\n",
    "           'results/inference_output_Video_1.mp4', \n",
    "           'results/inference_output_Video_2.mp4', \n",
    "           'results/Statistics.mp4'],\n",
    "           'results/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
