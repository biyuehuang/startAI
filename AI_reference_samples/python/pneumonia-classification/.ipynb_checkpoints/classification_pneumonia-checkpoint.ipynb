{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pneumonia detection in chest X-ray images\n",
    "\n",
    "Pneumonia is an inflammatory condition of the lung affecting primarily the small air sacs known as alveoli.\n",
    "Typically symptoms include some combination of cough, chest pain, fever, and trouble breathing [1].\n",
    "Pneumonia affects approximately 450 million people globally (7% of the population) and results in about 4 million deaths per year [2]. To diagnose this disease, chest X-ray images remain the best diagnosis tool.\n",
    "\n",
    "![](example-pneumonia.jpeg) *Chest X-ray image of a patient with Pneumonia*\n",
    "\n",
    "In this tutorial, we use a model trained to classify patients with pneumonia over healthy cases based on their chest X-ray images. The topology used is the DenseNet 121, this architecture has shown to be very efficient at this problem, it was the first work to claim classification rate better than practicing radiologists. The dataset used for training is from the \"Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images for Classification\" [3] with a CC BY 4.0 license.\n",
    "The trained model is provided as a frozen Tensorflow model (.pb).\n",
    "\n",
    "\n",
    "**In this tutorial, we perform inference on this model using the OpenVINO(TM) toolkit on the validation dataset on multiple hardware targets such as different Intel CPUS, Neural Compute Stick, the Intel® Arria® 10 FPGA, etc...\n",
    "We will also visualize what the network has learned using the technique of Class Activation Maps.**\n",
    "\n",
    "[1] Ashby B, Turkington C (2007). The encyclopedia of infectious diseases (3rd ed.). New York: Facts on File. p. 242. ISBN 978-0-8160-6397-0. \n",
    "\n",
    "[2] Ruuskanen O, Lahti E, Jennings LC, Murdoch DR (April 2011). \"Viral pneumonia\". Lancet. 377 (9773): 1264–75. \n",
    "\n",
    "[3] Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images for Classification, http://dx.doi.org/10.17632/rscbjbr9sj.2#file-41d542e7-7f91-47f6-9ff2-dd8e5a5a7861"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to convert the model to OpenVINO Intermediate Representation through the Model Optimizer. The MO script is *mo_tf.py*, which takes as input:  \n",
    "-m: path to the input model (.pb)  \n",
    "\n",
    "--input_shape: dimension of the input tensor (optional for most of the models)\n",
    "\n",
    "--data_type: data type of the IR model (FP32 for CPU, FP16 for GPU, Movidius and FPGA)\n",
    "\n",
    "-o : path where to save the IR\n",
    "\n",
    "--mean_values : pre-processing mean values with which the model was trained with \n",
    "\n",
    "--scale_values : pre-processing scale values with which the model was trained with \n",
    "\n",
    "The second argument is not necessary for  models with a defined input shape. However, Tensorflow models often have dynamic input shapes (often the batch will be set as -1), which is not supported by OpenVINO.\n",
    "The mean and scale values are also optional, it however simplifies the inference later as it includes automatically the pre-processing (scaling and averaging) inside the model directly. For this model, the model was trained with images following the ImageNet pre-processing. \n",
    "\n",
    "First, we convert the model to a FP32 IR format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py -m model.pb --input_shape=[1,224,224,3] --data_type FP32 -o models/FP32 --mean_values [123.75,116.28,103.58] --scale_values [58.395,57.12,57.375] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also convert to FP16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py -m model.pb --input_shape=[1,224,224,3] --data_type FP16 -o models/FP16/ --mean_values [123.75,116.28,103.58] --scale_values [58.395,57.12,57.375]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the result Intermediate Representation (.xml). You can observe the different layers with informations such as name, operation type, precision, input shape and output shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import bs4\n",
    "    print(\"bs4 already installed\")\n",
    "except:\n",
    "    print(\"Installing bs4\")\n",
    "    !pip3 install --user bs4\n",
    "    \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "bs = BeautifulSoup(open('models/FP32/model.xml'), 'xml')\n",
    "print(bs.prettify()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we are able to perform inference with OpenVINO. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inference with OpenVINO on the server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will perform inference on the server (Xeon SkyLake) in order to showcase the Python API of the OpenVINO inference engine. \n",
    "Then, we will showcase how to use our API to compute the Class Activation Maps. \n",
    "In Section 3, we will perform inference on other hardware target devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import several useful Python packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os,glob\n",
    "import numpy as np\n",
    "import logging as log\n",
    "from time import time\n",
    "from openvino.inference_engine import IENetwork, IEPlugin\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set several variables such as path to the IR model, the target device and path to the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xml = \"models/FP32/model.xml\"\n",
    "device=\"CPU\"\n",
    "files_pneumonia=glob.glob(os.getcwd()+'/validation_images/PNEUMONIA/*.jpeg')\n",
    "files_healthy=glob.glob(os.getcwd()+'/validation_images/NORMAL/*.jpeg')\n",
    "colormap='viridis'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define an image reading function that uses OpenCV for image loading, resize it to the network input size, and reshape the image numpy array into CHW (Channels, Height, Width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_img, img_to_array, resize_image\n",
    "\n",
    "def read_image(path):\n",
    "    image_original = load_img(path, color_mode=\"rgb\")\n",
    "    img= resize_image(image_original, target_size=(224, 224))\n",
    "    x = img_to_array(img, data_format='channels_first')\n",
    "    return [x,image_original]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to perform the classification of the pneumonia cases over the healthy patients.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bin = os.path.splitext(model_xml)[0] + \".bin\"\n",
    "\n",
    "# Plugin initialization for specified device and load extensions library if specified\n",
    "plugin = IEPlugin(device=device)\n",
    "\n",
    "# Read IR\n",
    "net = IENetwork(model=model_xml, weights=model_bin)\n",
    "\n",
    "assert len(net.inputs.keys()) == 1, \"Sample supports only single input topologies\"\n",
    "\n",
    "input_blob = next(iter(net.inputs))\n",
    "out_blob = next(iter(net.outputs))\n",
    "\n",
    "net.batch_size = 1\n",
    "\n",
    "exec_net = plugin.load(network=net)\n",
    "\n",
    "n,c,h,w=net.inputs[input_blob].shape\n",
    "case=0\n",
    "\n",
    "print(\"Pneumomina cases\")\n",
    "for file in sorted(files_pneumonia):\n",
    "    [image1,image]= read_image(file)\n",
    "    t0 = time()\n",
    "    res = exec_net.infer(inputs={input_blob: image1})\n",
    "    infer_time=(time() - t0) * 1000\n",
    "    res_pb = res[out_blob]\n",
    "    probs=res_pb[0][0]\n",
    "    plt.imshow(image)\n",
    "    plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "    plt.show()\n",
    "    print(\"Probability of having disease= \"+str(probs)+\" (ground truth: pneumonia case), performed in \" + str(infer_time) +\" ms\" )\n",
    "        \n",
    "print(\"Healthy cases\")        \n",
    "for file in sorted(files_healthy):\n",
    "    [image1,image]= read_image(file)\n",
    "    t0 = time()\n",
    "    res = exec_net.infer(inputs={input_blob: image1})\n",
    "    infer_time=(time() - t0) * 1000\n",
    "    res_pb = res[out_blob]\n",
    "    probs=res_pb[0][0]\n",
    "    plt.imshow(image)\n",
    "    plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "    plt.show()\n",
    "    print(\"Probability of having disease= \"+str(probs)+\" (ground truth: healthy case), performed in \" + str(infer_time) +\" ms\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualize what the model has learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class activation maps (CAM) [1] are a simple technique to visualize the regions that are relevant to a Convolutional Neural Network to identify a specific class in the image. \n",
    "The CAM $M(x,y)$ is calculated from the N feature maps $f_i(x,y)$ from the last convolutional layer. We perform the weighted sum of those feature maps based on the weigths of the fully connected layer $w_i$ , which represents how important are those feaure maps for the classification output. \n",
    "\n",
    "$ M(x,y)=\\sum_{i=0}^N w_i f_i(x,y) $\n",
    "\n",
    "[1] Zhou, Bolei, et al. \"Learning deep features for discriminative localization.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate Class Activation Maps in OpenVINO, we need to access the output feature maps of the last convolution layer and the weights of the fully connected layer. \n",
    "By default, only the output layer is the output (obviously). Therefore, in order to get the feature maps, we need to add the last convolution as an additional output. \n",
    "This is simply done by using the function *add_outputs(\"layer_name\")* on the network before loading it to the plugin. \n",
    "In order to obtain the layers name, we recommend Netron, which allows to visualize graphically the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the feature maps, inference must be performed first. This is why our function to calculate the Class Action Maps requires the inference output (which includes the feature maps since we added an additional output previously).\n",
    "We access the FC layer weights using the call *net.layers.get(\"name_of_fully_connected_layer\").weights[\"weights\"]*\n",
    "\n",
    "Then, we perform the weighted sum of the weights with the feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_class_activation_map_openvino(res, bn, fc, net):\n",
    "    res_bn = res[bn]\n",
    "    weights_fc=net.layers.get(fc).weights[\"weights\"]\n",
    "    conv_outputs=res_bn[0,:,:,:]\n",
    "    cam = np.zeros(dtype=np.float32, shape=(conv_outputs.shape[1:]))\n",
    "    for i, w in enumerate(weights_fc):\n",
    "        cam += w * conv_outputs[i, :, :]\n",
    "    return cam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now defined the generic function in order to obtain the Class Activation Maps, let's use it now for our example. \n",
    "\n",
    "We define first the name of the last convolutional layer from which we need to extract the feature maps. In our example, it is the layer with ID=365. \n",
    "We also set the name of the Fully-Connected layer, in our case **predictions_1/MatMul**\n",
    "\n",
    "For our tutorial, we parse the xml in order to get the name of the last Convolutional layer but you can also directly provide it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing the IR in order to find the last convolutional layer \n",
    "from bs4 import BeautifulSoup\n",
    "bs = BeautifulSoup(open('models/FP32/model.xml'), 'xml')\n",
    "bnTag = bs.findAll(attrs={\"id\" : \"365\"})\n",
    "bn = bnTag[0]['name']\n",
    "print(bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the last convolutional layer as output \n",
    "net.add_outputs(bn)\n",
    "fc=\"predictions_1/MatMul\"\n",
    "\n",
    "# name of the inputs and outputs\n",
    "input_blob = next(iter(net.inputs))\n",
    "out_blob = \"predictions_1/Sigmoid\"\n",
    "\n",
    "#set the batch size\n",
    "net.batch_size = 1\n",
    "\n",
    "# Loading model to the plugin\n",
    "print(\"Loading model to the plugin\")\n",
    "exec_net = plugin.load(network=net)\n",
    "\n",
    "#obtain the shape of the input\n",
    "n,c,h,w=net.inputs[input_blob].shape\n",
    "\n",
    "#iterate over the pneumonia cases\n",
    "for file in sorted(files_pneumonia):\n",
    "    #read the image\n",
    "    [image1,image]= read_image(file)\n",
    "    # Start inference\n",
    "    t0 = time()\n",
    "    res = exec_net.infer(inputs={input_blob: image1})\n",
    "    infer_time=(time() - t0) * 1000\n",
    "    \n",
    "    # Process the classification output\n",
    "    res_pb = res[out_blob]\n",
    "    probs=res_pb[0]\n",
    "    print(\"Probability of having disease= \"+str(probs)+\" (ground truth: pneumonia case), performed in \" + str(infer_time) +\" ms\")\n",
    "    \n",
    "    # Class Activation Map    \n",
    "    t0 = time()\n",
    "    cam=visualize_class_activation_map_openvino(res, bn, fc , net)\n",
    "    cam_time=(time() - t0) * 1000\n",
    "    print(\"Time for CAM: {} ms\".format(cam_time))\n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    # Visualize the CAM heatmap\n",
    "    cam /= np.max(cam)\n",
    "    fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(cam, cmap=colormap)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Visualize the CAM overlaid over the X-ray image \n",
    "    colormap_val=cm.get_cmap(colormap)  \n",
    "    imss=np.uint8(colormap_val(cam)*255)\n",
    "    im = Image.fromarray(imss)\n",
    "    width, height = image.size\n",
    "    cam1=resize_image(im, (height,width))\n",
    "    heatmap = np.asarray(cam1)\n",
    "    img1 = heatmap [:,:,:3] * 0.3 + image\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    \n",
    "    \n",
    "    plt.imshow(np.uint16(img1))\n",
    "    plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference on the edge\n",
    "\n",
    "\n",
    "All the code up to this point has been run within the Jupyter Notebook instance running on a development node based on an Intel Core processor, where the Notebook is allocated a single core. \n",
    "We will run the workload on CPU and VPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 RUN on CPU \n",
    "\n",
    "Run below code on CPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path().resolve().parent.parent))\n",
    "from demoTools.demoutils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference on CPU with classification_pneumonia.py\n",
    "!python3 classification_pneumonia.py  -m models/FP32/model.xml  \\\n",
    "                                           -i /validation_images/PNEUMONIA/*.jpeg \\\n",
    "                                           -o results \\\n",
    "                                           -d CPU\n",
    "                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 RUN the same case on VPU\n",
    " \n",
    "Only run below code if there is a VPU attach to your device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference on CPU with classification_pneumonia.py\n",
    "#!python3 classification_pneumonia.py  -m models/FP32/model.xml  \\\n",
    "#                                           -i /validation_images/PNEUMONIA/*.jpeg \\\n",
    "#                                           -o results/VPU \\\n",
    "#                                           -d MYRIAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 View Results\n",
    "\n",
    "Once the jobs are completed, the queue system outputs the stdout and stderr streams of each job into files with names of the form\n",
    "\n",
    "`obj_det_{type}.o{JobID}`\n",
    "\n",
    "`obj_det_{type}.e{JobID}`\n",
    "\n",
    "(here, obj_det_{type} corresponds to the `-N` option of qsub).\n",
    "\n",
    "We also saved the probability output and inference time for each input image in the folder `results/` for each architecture. \n",
    "We observe the results below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result on the Intel Core CPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file=\"results/result58593.c003.txt\"\n",
    "file_ready= os.path.isfile(result_file)\n",
    "if file_ready:\n",
    "    f=open(result_file)\n",
    "    print(f.read())\n",
    "else: \n",
    "    print(\"The results are not ready yet, please retry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result on the Intel NCS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Only run below if you  have a VPU attached to your device\n",
    "\n",
    "#result_file=\"results/CPU/result58593.c003.txt.txt\"\n",
    "#file_ready= os.path.isfile(result_file)\n",
    "#if file_ready:\n",
    "#    f=open(result_file)\n",
    "#    print(f.read())\n",
    "#else: \n",
    "#    print(\"The results are not ready yet, please retry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
